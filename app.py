import json
import io
from typing import List, Dict, Tuple

import pandas as pd
import numpy as np
import altair as alt
import streamlit as st
from pathlib import PurePosixPath

st.set_page_config(page_title="Twinkle Eval Analyzer", page_icon=":star2:", layout="wide")

st.title("âœ¨ Twinkle Eval Analyzer (.json / .jsonl)")

# ----------------- Helpers -----------------

def _decode_bytes_to_text(b: bytes) -> str:
    for enc in ("utf-8", "utf-16", "utf-16le", "utf-16be", "big5", "cp950"):
        try:
            return b.decode(enc)
        except Exception:
            continue
    return b.decode("utf-8", errors="ignore")

def read_twinkle_doc(file) -> Dict:
    raw = file.read()
    if isinstance(raw, bytes):
        text = _decode_bytes_to_text(raw)
    else:
        text = raw
    text = text.strip()
    try:
        obj = json.loads(text)
    except Exception:
        for line in text.splitlines():
            line = line.strip().rstrip(",")
            if not line:
                continue
            try:
                obj = json.loads(line)
                break
            except Exception:
                continue
    if not isinstance(obj, dict):
        raise ValueError("æª”æ¡ˆä¸æ˜¯æœ‰æ•ˆçš„ Twinkle Eval JSON ç‰©ä»¶ã€‚")
    if "timestamp" not in obj or "config" not in obj or "dataset_results" not in obj:
        raise ValueError("ç¼ºå°‘å¿…è¦æ¬„ä½")
    return obj

def extract_records(doc: Dict) -> Tuple[pd.DataFrame, Dict[str, float]]:
    model = doc.get("config", {}).get("model", {}).get("name", "<unknown>")
    timestamp = doc.get("timestamp", "<no-ts>")
    source_label = f"{model} @ {timestamp}"
    rows = []
    avg_map = {}
    for ds_path, ds_payload in doc.get("dataset_results", {}).items():
        ds_name = ds_path.split("datasets/")[-1].strip("/") if ds_path.startswith("datasets/") else ds_path
        avg_meta = ds_payload.get("average_accuracy") if isinstance(ds_payload, dict) else None
        results = ds_payload.get("results", []) if isinstance(ds_payload, dict) else []
        for item in results:
            if not isinstance(item, dict):
                continue
            file_path = item.get("file")
            acc_mean = item.get("accuracy_mean")
            if file_path is None or acc_mean is None:
                continue
            fname = PurePosixPath(file_path).name
            category = fname.rsplit(".", 1)[0]
            rows.append({
                "dataset": ds_name,
                "category": category,
                "file": fname,
                "accuracy_mean": float(acc_mean),
                "source_label": source_label
            })
        if avg_meta is None and results:
            vals = [float(it.get("accuracy_mean", np.nan)) for it in results if "accuracy_mean" in it]
            if vals:
                avg_meta = float(np.mean(vals))
        if avg_meta is not None:
            avg_map[ds_name] = avg_meta
    return pd.DataFrame(rows), avg_map

def load_all(files) -> Tuple[pd.DataFrame, Dict[str, Dict[str, float]]]:
    frames = []
    meta = {}
    for f in files or []:
        try:
            doc = read_twinkle_doc(f)
        except Exception as e:
            st.error(f"âŒ ç„¡æ³•è®€å– {getattr(f, 'name', 'æª”æ¡ˆ')}ï¼š{e}")
            continue
        df, avg_map = extract_records(doc)
        if not df.empty:
            frames.append(df)
            src = df["source_label"].iloc[0]
            meta[src] = avg_map
    if not frames:
        return pd.DataFrame(columns=["dataset", "category", "file", "accuracy_mean", "source_label"]), {}
    return pd.concat(frames, ignore_index=True), meta

# ----------------- Sidebar -----------------

with st.sidebar:
    files = st.file_uploader("é¸æ“‡ Twinkle Eval æª”æ¡ˆ", type=["json", "jsonl"], accept_multiple_files=True)
    df_all, meta_all = load_all(files)
    normalize_0_100 = st.checkbox("ä»¥ 0â€“100 é¡¯ç¤º", value=False)
    page_size = st.selectbox("æ¯å¼µåœ–é¡¯ç¤ºå¹¾å€‹é¡åˆ¥", [10, 20, 30, 50, 100], index=1)
    sort_mode = st.selectbox("æ’åºæ–¹å¼ï¼ˆåŸå§‹æˆç¸¾ï¼‰", ["ä¾æ•´é«”å¹³å‡ç”±é«˜åˆ°ä½", "ä¾æ•´é«”å¹³å‡ç”±ä½åˆ°é«˜", "ä¾å­—æ¯æ’åº"])

    # === Baseline Î” åœ–è¡¨çš„æ§åˆ¶ ===
    st.markdown("---")
    st.subheader("å·®è·åˆ†æè¨­å®šï¼ˆBaseline Î”ï¼‰")
    options = ["|Î”| ç”±å¤§åˆ°å°", "Î” ç”±å¤§åˆ°å°ï¼ˆæå‡æœ€å¤šï¼‰", "Î” ç”±å°åˆ°å¤§ï¼ˆä¸‹é™æœ€å¤šï¼‰", "ä¾é¡åˆ¥åç¨±"]
    default = "Î” ç”±å¤§åˆ°å°ï¼ˆæå‡æœ€å¤šï¼‰"
    delta_sort_mode = st.selectbox("å·®è·æ’åºæ–¹å¼ï¼ˆper-categoryï¼‰", options, index=options.index(default), key="delta_sort_mode")

    abs_threshold = st.number_input("åªé¡¯ç¤º |Î”| â‰¥ é–€æª»ï¼ˆå¯é¸ï¼‰", min_value=0.0, value=0.0, step=0.1)
    st.caption("Î” = Candidate åˆ†æ•¸ âˆ’ Baseline åˆ†æ•¸ï¼›å»ºè­°ä»¥ 0â€“100 æ¨¡å¼è¨ˆç®—æ›´ç›´è§€ã€‚")

if df_all.empty:
    st.info("è«‹ä¸Šå‚³ Twinkle Eval æª”æ¡ˆ")
    st.stop()

# ----------------- åŸå§‹æˆç¸¾-----------------
all_datasets = sorted(df_all["dataset"].unique().tolist())
selected_dataset = st.selectbox("é¸æ“‡è³‡æ–™é›†", options=all_datasets)
work = df_all[df_all["dataset"] == selected_dataset].copy()
metric_plot = "accuracy_mean" + (" (x100)" if normalize_0_100 else "")
work[metric_plot] = work["accuracy_mean"] * (100.0 if normalize_0_100 else 1.0)

order_df = work.groupby("category")[metric_plot].mean().reset_index()
if sort_mode == "ä¾æ•´é«”å¹³å‡ç”±é«˜åˆ°ä½":
    order_df = order_df.sort_values(metric_plot, ascending=False)
elif sort_mode == "ä¾æ•´é«”å¹³å‡ç”±ä½åˆ°é«˜":
    order_df = order_df.sort_values(metric_plot, ascending=True)
else:
    order_df = order_df.sort_values("category", ascending=True)

cat_order = order_df["category"].tolist()
work["category"] = pd.Categorical(work["category"], categories=cat_order, ordered=True)

n = len(cat_order)
pages = int(np.ceil(n / page_size))

st.markdown("## ğŸ“ˆ åŸå§‹æˆç¸¾ï¼ˆå„æ¨¡å‹ Ã— é¡åˆ¥ï¼‰")
for p in range(pages):
    start, end = p * page_size, min((p + 1) * page_size, n)
    subset_cats = cat_order[start:end]
    sub = work[work["category"].isin(subset_cats)]
    st.subheader(f"ğŸ“Š {selected_dataset}ï½œé¡åˆ¥ {start+1}-{end} / {n}")
    base = alt.Chart(sub).encode(
        x=alt.X("category:N", sort=subset_cats),
        y=alt.Y(f"{metric_plot}:Q"),
        color=alt.Color("source_label:N"),
        tooltip=["source_label", "file", alt.Tooltip(metric_plot, format=".3f")]
    )
    bars = base.mark_bar().encode(xOffset="source_label")
    st.altair_chart(bars.properties(height=420), use_container_width=True)
    pivot = sub.pivot_table(index="category", columns="source_label", values=metric_plot)
    st.dataframe(pivot, use_container_width=True)
    st.download_button(
        label=f"ä¸‹è¼‰æ­¤é  CSV ({start+1}-{end})",
        data=pivot.reset_index().to_csv(index=False).encode("utf-8"),
        file_name=f"twinkle_{selected_dataset}_{start+1}_{end}.csv",
        mime="text/csv"
    )

# ----------------- å·®è·ï¼ˆBaseline Î”ï¼‰åˆ†æ -----------------

st.markdown("---")
st.markdown("## âš–ï¸ å·®è·åˆ†æï¼šBaseline vs. Candidatesï¼ˆÎ” = Candidate âˆ’ Baselineï¼‰")

# ä½¿ç”¨èˆ‡ä¸Šæ–¹ç›¸åŒçš„è³‡æ–™é›†
dataset_for_delta = selected_dataset

df_delta_scope = df_all[df_all["dataset"] == dataset_for_delta].copy()
if df_delta_scope.empty:
    st.warning(f"åœ¨è³‡æ–™é›† **{dataset_for_delta}** æ‰¾ä¸åˆ°è³‡æ–™ï¼Œè«‹ç¢ºèªä¸Šå‚³çš„ JSON å«æ­¤è³‡æ–™é›†åç¨±ã€‚")
    try:
        st.stop()
    except Exception:
        raise SystemExit

# çµ±ä¸€èˆ‡ä¸Šæ–¹å°ºåº¦ï¼ˆå»ºè­°ç”¨ 0â€“100 å†åšå·®ï¼‰
score_col = "score_0100"
df_delta_scope[score_col] = df_delta_scope["accuracy_mean"] * (100.0 if normalize_0_100 else 1.0)

# æ‰‹å‹•æŒ‡å®š Baseline èˆ‡ Candidates
all_sources_in_scope = sorted(df_delta_scope["source_label"].unique().tolist())
col1, col2 = st.columns([1, 2])
with col1:
    baseline = st.selectbox("é¸æ“‡åŸºæº–æ¨¡å‹ï¼ˆBaselineï¼‰", options=all_sources_in_scope)
with col2:
    default_candidates = [s for s in all_sources_in_scope if s != baseline]
    candidates = st.multiselect("é¸æ“‡è¦æ¯”è¼ƒçš„å€™é¸æ¨¡å‹ï¼ˆCandidatesï¼‰", options=all_sources_in_scope, default=default_candidates)

if not candidates:
    st.info("è«‹è‡³å°‘é¸æ“‡ä¸€å€‹ Candidateã€‚")
    try:
        st.stop()
    except Exception:
        raise SystemExit

# å»ºç«‹å¯¬è¡¨ï¼ˆindex=categoryï¼›å·²å›ºå®š dataset_for_deltaï¼‰
wide = df_delta_scope.pivot_table(index="category", columns="source_label", values=score_col, aggfunc="mean")

# åªæ¯”è¼ƒ baseline èˆ‡ candidates çš„äº¤é›†åˆ—
valid_candidates = [c for c in candidates if c in wide.columns]
if baseline not in wide.columns:
    st.error("Baseline åœ¨æ­¤è³‡æ–™é›†æ²’æœ‰ä»»ä½•åˆ†æ•¸å¯æ¯”ã€‚è«‹æ›ä¸€å€‹ Baseline æˆ–è³‡æ–™é›†ã€‚")
    try:
        st.stop()
    except Exception:
        raise SystemExit
if not valid_candidates:
    st.error("é¸å–çš„ Candidates åœ¨æ­¤è³‡æ–™é›†æ²’æœ‰ä»»ä½•åˆ†æ•¸å¯æ¯”ã€‚è«‹æ›ä¸€çµ„ Candidates æˆ–è³‡æ–™é›†ã€‚")
    try:
        st.stop()
    except Exception:
        raise SystemExit

# è¨ˆç®— Î” é•·è¡¨ï¼ˆä¿ç•™ baseline/candidate åŸå§‹åˆ†æ•¸ï¼‰
delta_rows = []
for c in valid_candidates:
    pair = wide[[baseline, c]].dropna()  # åƒ…å…©è€…çš†æœ‰åˆ†æ•¸çš„é¡åˆ¥
    if pair.empty:
        continue
    for cat, row in pair.iterrows():
        b = float(row[baseline])
        s = float(row[c])
        delta = s - b
        if abs(delta) < abs_threshold:  # é–€æª»éæ¿¾
            continue
        delta_rows.append({
            "dataset": dataset_for_delta,
            "category": cat,
            "baseline": baseline,
            "candidate": c,
            "baseline_score": b,
            "candidate_score": s,
            "delta": delta
        })

delta_df = pd.DataFrame(delta_rows)
if delta_df.empty:
    st.warning("æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„å¯æ¯”è¼ƒé¡åˆ¥ï¼ˆå¯èƒ½å› ç¼ºæ¼æˆ–é–€æª»éé«˜ï¼‰ã€‚")
    try:
        st.stop()
    except Exception:
        raise SystemExit

# å·®è·æ’åº
if delta_sort_mode == "|Î”| ç”±å¤§åˆ°å°":
    delta_df = delta_df.sort_values("delta", key=lambda s: s.abs(), ascending=False)
elif delta_sort_mode == "Î” ç”±å¤§åˆ°å°ï¼ˆæå‡æœ€å¤šï¼‰":
    delta_df = delta_df.sort_values("delta", ascending=False)
elif delta_sort_mode == "Î” ç”±å°åˆ°å¤§ï¼ˆä¸‹é™æœ€å¤šï¼‰":
    delta_df = delta_df.sort_values("delta", ascending=True)
else:
    delta_df = delta_df.sort_values("category", ascending=True)

# åœ–è¡¨ï¼ˆÎ” ä¸åˆ†é ï¼Œä¸€æ¬¡é¡¯ç¤ºå…¨éƒ¨é¡åˆ¥ï¼‰
tab1, tab2 = st.tabs(["ğŸ“Š å·®è·æ’è¡Œï¼ˆper-categoryï¼‰", "ğŸ“œ æ¨¡å‹ç¸½çµï¼ˆper-candidateï¼‰"])

with tab1:
    sub = delta_df.copy()

    # === å…ˆåœ¨ Pandas å…§ç®—å‡ºæ¯å€‹ candidate çš„æ’åºåæ¬¡ ===
    if delta_sort_mode == "Î” ç”±å¤§åˆ°å°ï¼ˆæå‡æœ€å¤šï¼‰":
        sub["rank_in_candidate"] = sub.groupby("candidate")["delta"].rank(ascending=False, method="first")
        table_sort = lambda df: df.sort_values(["candidate", "rank_in_candidate"], ascending=[True, True])
        y_sort = alt.SortField("rank_in_candidate", order="ascending")
        resolve_y = "independent"

    elif delta_sort_mode == "Î” ç”±å°åˆ°å¤§ï¼ˆä¸‹é™æœ€å¤šï¼‰":
        sub["rank_in_candidate"] = sub.groupby("candidate")["delta"].rank(ascending=True, method="first")
        table_sort = lambda df: df.sort_values(["candidate", "rank_in_candidate"], ascending=[True, True])
        y_sort = alt.SortField("rank_in_candidate", order="ascending")
        resolve_y = "independent"

    elif delta_sort_mode == "|Î”| ç”±å¤§åˆ°å°":
        sub["abs_delta"] = sub["delta"].abs()
        sub["rank_in_candidate"] = sub.groupby("candidate")["abs_delta"].rank(ascending=False, method="first")
        table_sort = lambda df: df.sort_values(["candidate", "rank_in_candidate"], ascending=[True, True])
        y_sort = alt.SortField("rank_in_candidate", order="ascending")
        resolve_y = "independent"

    else:  # ä¾é¡åˆ¥åç¨±ï¼ˆå­—æ¯åºï¼‰ï¼Œå…±ç”¨æ’åº
        # ä¸ç”¨ rankï¼Œç›´æ¥å­—æ¯åº
        table_sort = lambda df: df.sort_values(["category", "candidate"], ascending=[True, True])
        y_sort = alt.SortField("category", order="ascending")
        resolve_y = "shared"

    st.subheader(f"ğŸ” {dataset_for_delta}ï½œÎ” æ’è¡Œï¼ˆå…¨éƒ¨ {sub['category'].nunique()} é¡åˆ¥ï¼‰")

    chart_height = 25 * max(1, sub["category"].nunique())

    base = alt.Chart(sub).encode(
        y=alt.Y("category:N", sort=y_sort, title="Category"),
        x=alt.X("delta:Q", title="Î” = Candidate âˆ’ Baseline"),
        color=alt.Color("candidate:N", title="Candidate"),
        tooltip=[
            alt.Tooltip("category:N", title="Category"),
            alt.Tooltip("candidate:N", title="Candidate"),
            alt.Tooltip("baseline:N", title="Baseline"),
            alt.Tooltip("baseline_score:Q", title="Baseline åˆ†æ•¸", format=".3f"),
            alt.Tooltip("candidate_score:Q", title="Candidate åˆ†æ•¸", format=".3f"),
            alt.Tooltip("delta:Q", title="Î”", format=".3f"),
        ],
    )

    chart = (
        base.mark_bar()
        .encode(row=alt.Row("candidate:N", header=alt.Header(title=None)))
        .properties(height=chart_height)
        .resolve_scale(y=resolve_y)  # å„ candidate åˆ†é¢å„è‡ªæ’åºæˆ–å…±ç”¨
    )
    st.altair_chart(chart, use_container_width=True)

    # è¡¨æ ¼ï¼šä¾ rank_in_candidate æ’åºï¼Œèˆ‡åœ–ä¸€è‡´
    table = table_sort(sub)[["category", "candidate", "baseline_score", "candidate_score", "delta"]]
    st.dataframe(table, use_container_width=True)

    st.download_button(
        label="ä¸‹è¼‰ Î” æ’è¡Œ CSVï¼ˆå…¨éƒ¨é¡åˆ¥ï¼‰",
        data=table.to_csv(index=False).encode("utf-8"),
        file_name=f"delta_{dataset_for_delta}_ALL.csv",
        mime="text/csv",
    )



with tab2:
    # per-candidate ç¸½çµï¼šmean/median Î”ã€win/lose/tieã€è¦†è“‹ç‡ã€Top/Bottom-N
    summaries = []
    top_k = st.number_input("Top/Bottom-Nï¼ˆé¡¯ç¤ºæ¯å€‹ Candidate çš„æœ€å¤§/æœ€å°å·®è·åˆ†é¡ï¼‰", min_value=1, value=10, step=1)

    for c in valid_candidates:
        pair = wide[[baseline, c]].dropna()
        if pair.empty:
            continue
        deltas = pair[c] - pair[baseline]
        m = float(np.mean(deltas))
        med = float(np.median(deltas))
        win = int((deltas > 0).sum())
        lose = int((deltas < 0).sum())
        tie = int((deltas == 0).sum())
        coverage = f"{len(deltas)}/{wide.shape[0]}"  # æœ‰å…±åŒåˆ†æ•¸çš„é¡åˆ¥æ•¸ / å…¨éƒ¨é¡åˆ¥æ•¸

        # å– Top/Bottom-N é¡åˆ¥ï¼ˆæŒ‰ Î”ï¼‰
        top_rows = (pair.assign(delta=deltas)
                    .sort_values("delta", ascending=False)
                    .head(top_k)
                    .reset_index()[["category", baseline, c, "delta"]])
        bottom_rows = (pair.assign(delta=deltas)
                       .sort_values("delta", ascending=True)
                       .head(top_k)
                       .reset_index()[["category", baseline, c, "delta"]])

        summaries.append({
            "candidate": c,
            "mean_delta": m,
            "median_delta": med,
            "win": win,
            "lose": lose,
            "tie": tie,
            "coverage": coverage,
            "top_list": top_rows,
            "bottom_list": bottom_rows
        })

    if not summaries:
        st.warning("æ²’æœ‰å¯ç”¨çš„ per-candidate ç¸½çµï¼ˆå¯èƒ½éƒ½æ²’æœ‰äº¤é›†ï¼‰ã€‚")
    else:
        # æ¦‚è¦½è¡¨
        overview = pd.DataFrame([{
            "Candidate": s["candidate"],
            "Mean Î”": s["mean_delta"],
            "Median Î”": s["median_delta"],
            "Win": s["win"],
            "Lose": s["lose"],
            "Tie": s["tie"],
            "Coverage (äº¤é›†/ç¸½é¡åˆ¥)": s["coverage"],
        } for s in summaries]).sort_values("Mean Î”", ascending=False)
        st.markdown("### ç¸½è¦½ï¼ˆèˆ‡ Baseline æˆå°æ¯”è¼ƒï¼‰")
        st.dataframe(overview, use_container_width=True)
        st.download_button(
            label="ä¸‹è¼‰ per-candidate ç¸½è¦½ CSV",
            data=overview.to_csv(index=False).encode("utf-8"),
            file_name=f"delta_overview_{dataset_for_delta}.csv",
            mime="text/csv"
        )

        # é€ Candidate é¡¯ç¤º Top/Bottom-N æ¸…å–®ï¼ˆå¯æ”¶åˆï¼‰
        st.markdown("### å„ Candidate çš„å·®è·æ¸…å–®ï¼ˆTop/Bottom-Nï¼‰")
        for s in summaries:
            with st.expander(f"ğŸ”¸ {s['candidate']}"):
                st.write("**Top-Nï¼ˆæå‡æœ€å¤šï¼‰**")
                top_tbl = s["top_list"].rename(columns={baseline: "baseline_score", s["candidate"]: "candidate_score"})
                st.dataframe(top_tbl, use_container_width=True)
                st.download_button(
                    label=f"ä¸‹è¼‰ {s['candidate']} Top-N",
                    data=top_tbl.to_csv(index=False).encode("utf-8"),
                    file_name=f"delta_top_{dataset_for_delta}_{s['candidate']}.csv",
                    mime="text/csv"
                )

                st.write("**Bottom-Nï¼ˆä¸‹é™æœ€å¤šï¼‰**")
                bottom_tbl = s["bottom_list"].rename(columns={baseline: "baseline_score", s["candidate"]: "candidate_score"})
                st.dataframe(bottom_tbl, use_container_width=True)
                st.download_button(
                    label=f"ä¸‹è¼‰ {s['candidate']} Bottom-N",
                    data=bottom_tbl.to_csv(index=False).encode("utf-8"),
                    file_name=f"delta_bottom_{dataset_for_delta}_{s['candidate']}.csv",
                    mime="text/csv"
                )
